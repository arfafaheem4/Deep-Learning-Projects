# -*- coding: utf-8 -*-
"""Bike Rental Prediction App using ANN.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1i3IeRgML7kOFTLfLV_erdvzVcb0vCgEp
"""

#Task 1
import pandas as pd
import numpy as np

df=pd.read_csv("bike_sharing_data.csv")
print(df.head())

print("\nMissing values: ", df.isnull().sum())
df=df.drop(['instant', 'dteday', 'casual', 'registered'], axis=1) #useless columns are dropped, cnt = casual + registered-> data leakage

import seaborn as sns
import matplotlib.pyplot as plt
print("\nHistogram...")
plt.figure()
df['cnt'].hist(bins=30)
plt.title("Historgram of bike rental count")
plt.xlabel("Count")
plt.ylabel("Frequency")
plt.show()

plt.figure(figsize=(10,8))
sns.heatmap(df.corr(), annot=True, cmap='coolwarm')
plt.title("Correlation heatmap")
plt.show()

categorical=['season','mnth','weekday', 'weathersit']
df=pd.get_dummies(df, columns=categorical, drop_first=True)
print(df.head())

X=df.drop('cnt', axis=1)
y=df['cnt']

from sklearn.model_selection import train_test_split
X_train, X_test, y_train, y_test = train_test_split(X,y, test_size=0.2, random_state=42)

from sklearn.preprocessing import StandardScaler
scaler=StandardScaler()
X_train_scaled=scaler.fit_transform(X_train)
X_test_scaled=scaler.transform(X_test)

#scaling ensures that all features remain on same scale if it is not on same scale,
#then features having large value dominate and training becomes unstable

#Task 2
from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import Dense, Dropout
from tensorflow.keras import regularizers

def build_ann_model(input_dim):
    model=Sequential()

    #input layer+hidden layer
    model.add(Dense(64,activation='relu', input_dim=input_dim, kernel_regularizer=regularizers.l2(0.01)))
    model.add(Dropout(0.2))

    #2ND hidden layer
    model.add(Dense(32, activation='relu', input_dim=input_dim, kernel_regularizer= regularizers.l2(0.01)))
    model.add(Dropout(0.2))

    #output layer
    model.add(Dense(1, activation='linear'))
    model.compile(optimizer='adam', loss='mean_squared_error', metrics=['mean_absolute_error'])
    return model

#building model
input_features= X_train_scaled.shape[1]  #no of columns
ann_model=build_ann_model(input_features)

# history=ann_model.fit(X_train_scaled, y_train, validation_split=0.2, epochs=100,batch_size=32,verbose=1) #for training just to check

#Task 3
from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import Dense, Dropout
from tensorflow.keras.optimizers import Adam
import numpy as np

no_of_neurons=[32,64,128]
learning_rate=[0.01,0.001]
batch_size=[32,64]
results=[]

for neurons in no_of_neurons:
    for lr in learning_rate:
      for bs in batch_size:
        model=Sequential()
        model.add(Dense(neurons, activation ='relu', input_dim= X_train_scaled.shape[1]))
        model.add(Dense(neurons//2, activation='relu'))
        model.add(Dense(1, activation='linear'))
        model.compile(optimizer=Adam(learning_rate=lr), loss='mean_squared_error', metrics=['mean_absolute_error'])
        history=model.fit(X_train_scaled, y_train,validation_split=0.2, epochs=50, batch_size=bs,verbose=0)
        val_loss= history.history['val_loss'][-1]
        results.append({
                'neurons': neurons,
                'learning_rate': lr,
                'batch_size': bs,
                'val_loss': val_loss
            })
        print(f"neurons:{neurons}, learning_rate:{lr}, batch_size:{bs}, val_loss:{val_loss}")
best=min(results, key=lambda x: x['val_loss'])
print("\nBest hyperparameters:", best)

#bayesian
!pip install optuna
import optuna
from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import Dense, Dropout
from tensorflow.keras.optimizers import Adam

def objective(trial):
    # Suggest hyperparameters
    neurons = trial.suggest_categorical('neurons', [32, 64, 128])
    lr = trial.suggest_float('learning_rate', 0.0001, 0.01, log=True)
    dropout = trial.suggest_float('dropout', 0.1, 0.5)
    batch_size = trial.suggest_categorical('batch_size', [32, 64, 128])

    model = Sequential()
    model.add(Dense(neurons, activation='relu', input_dim=X_train_scaled.shape[1]))
    model.add(Dropout(dropout))
    model.add(Dense(neurons//2, activation='relu'))
    model.add(Dropout(dropout))
    model.add(Dense(1, activation='linear'))

    model.compile(optimizer=Adam(learning_rate=lr), loss='mean_squared_error',metrics=['mean_absolute_error']
    )

    history = model.fit(X_train_scaled, y_train, validation_split=0.2,epochs=50,batch_size=batch_size,verbose=0)

    #Return final validation loss to minimize
    val_loss = history.history['val_loss'][-1]
    return val_loss

#Create Optuna study
study = optuna.create_study(direction='minimize')
study.optimize(objective, n_trials=20)  # n_trials = number of experiments

# Best hyperparameters
print("Best Hyperparameters:", study.best_params)
print("Best Validation Loss:", study.best_value)

#Task 4
from tensorflow.keras.callbacks import EarlyStopping
from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import Dense
from tensorflow.keras.optimizers import Adam

neuron = 64
learn_rate = 0.008635819301713123
drop = 0.1008
b_size = 64

final_model = Sequential()
final_model.add(Dense(neuron, activation='relu', input_dim=X_train_scaled.shape[1]))
final_model.add(Dropout(drop))
final_model.add(Dense(neuron//2, activation='relu'))
final_model.add(Dense(1, activation='linear'))

final_model.compile(optimizer=Adam(learning_rate=learn_rate),loss='mean_squared_error',metrics=['mean_absolute_error'])

early_stop =EarlyStopping(monitor='val_loss',patience=20,restore_best_weights=True)

history =final_model.fit(X_train_scaled,y_train,validation_split=0.2,epochs=200,batch_size=b_size,callbacks=[early_stop],verbose=1)
y_pred =final_model.predict(X_test_scaled)

from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score
import numpy as np

mae = mean_absolute_error(y_test, y_pred)
rmse = np.sqrt(mean_squared_error(y_test, y_pred))
r2 = r2_score(y_test, y_pred)

print("MAE:", mae)
print("RMSE:", rmse)
print("R2 Score:", r2)

import matplotlib.pyplot as plt

plt.figure(figsize=(6,6))
plt.scatter(y_test, y_pred)
plt.xlabel("Actual Values")
plt.ylabel("Predicted Values")
plt.title("Predictions vs Actual")
plt.plot([y_test.min(), y_test.max()],
         [y_test.min(), y_test.max()],
         'r')  # 45-degree line
plt.show()

print(y_train.min(), y_train.max())

!pip install gradio

import gradio as gr
import numpy as np
import pandas as pd

# Assume these are already loaded from your training
# final_model = <your trained ANN>
# scaler = <your trained StandardScaler>
# X_train = <training features DataFrame>

def predict_bikes(season, mnth, weekday, weathersit, temp, hum, windspeed):
    # Automatically set workingday and holiday
    holiday = 1 if weekday in [6,0] else 0  # weekend as holiday
    workingday = 0 if holiday==1 else 1

    # Create input dict with numeric features and one-hot for categorical
    input_dict = {
        'yr': 0,  # fixed year for simplicity
        'workingday': workingday,
        'holiday': holiday,
        'temp': temp,
        'hum': hum,
        'windspeed': windspeed,

        # Season one-hot (drop first column)
        'season_2': 1 if season==2 else 0,
        'season_3': 1 if season==3 else 0,
        'season_4': 1 if season==4 else 0,

        # Month one-hot (drop first)
        'mnth_2': 1 if mnth==2 else 0,
        'mnth_3': 1 if mnth==3 else 0,
        'mnth_4': 1 if mnth==4 else 0,
        'mnth_5': 1 if mnth==5 else 0,
        'mnth_6': 1 if mnth==6 else 0,
        'mnth_7': 1 if mnth==7 else 0,
        'mnth_8': 1 if mnth==8 else 0,
        'mnth_9': 1 if mnth==9 else 0,
        'mnth_10': 1 if mnth==10 else 0,
        'mnth_11': 1 if mnth==11 else 0,
        'mnth_12': 1 if mnth==12 else 0,

        # Weekday one-hot (drop first)
        'weekday_1': 1 if weekday==1 else 0,
        'weekday_2': 1 if weekday==2 else 0,
        'weekday_3': 1 if weekday==3 else 0,
        'weekday_4': 1 if weekday==4 else 0,
        'weekday_5': 1 if weekday==5 else 0,
        'weekday_6': 1 if weekday==6 else 0,

        # Weather one-hot (drop first)
        'weathersit_2': 1 if weathersit==2 else 0,
        'weathersit_3': 1 if weathersit==3 else 0
    }

    # Convert to DataFrame
    df_input = pd.DataFrame([input_dict])

    # Reindex to match training columns exactly
    df_input = df_input.reindex(columns=X_train.columns, fill_value=0)

    # Scale features
    input_scaled = scaler.transform(df_input)

    # Predict
    prediction = final_model.predict(input_scaled)
    return int(prediction[0][0])  # return as integer bike count

# Gradio interface
interface = gr.Interface(
    fn=predict_bikes,
    inputs=[
        gr.Number(label="Season (1=Spring, 2=Summer, 3=Fall, 4=Winter)"),
        gr.Number(label="Month (1-12)"),
        gr.Number(label="Weekday (0=Sun, 1=Mon ... 6=Sat)"),
        gr.Number(label="Weather (1=Clear, 2=Cloudy, 3=Rain/Snow)"),
        gr.Number(label="Temperature (Â°C)"),
        gr.Number(label="Humidity (%)"),
        gr.Number(label="Windspeed (km/h)")
    ],
    outputs=gr.Number(label="Predicted Bike Rentals"),
    title="Bike Rental Prediction App"
)

interface.launch()